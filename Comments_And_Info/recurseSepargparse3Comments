This bash script implements another recursive job submission system for the SLURM workload manager that automatically chains together multiple processing jobs in sequence, functioning identically to the recurseSepargparse2 script but for a different stage of the processing pipeline. The script requests minimal resources with only 10 CPU cores and a 1-minute time limit on the windfall partition, as its sole purpose is job orchestration rather than performing any computational work. When executed, the script takes a single integer argument representing an iteration counter, increments this value by one, and checks whether the current iteration number is less than or equal to a predefined limit. If the limit has not been reached, the script submits two new SLURM jobs: first it submits separgparse3.slurm with the current iteration number to execute the actual processing work for that batch at this particular pipeline stage, and then it recursively submits itself (recurseSepargparse3.slurm) with the incremented iteration number to perpetuate the job chain. This recursive submission mechanism enables automatic sequential processing of multiple batches without requiring manual job submission for each one, with each recurseSepargparse3 job spawning both the next processing job and the next recursive controller until all batches have been queued. Once the iteration counter exceeds the specified limit, the recursion terminates and the script prints a message indicating that the limit has been reached and no additional jobs will be submitted. This approach is particularly valuable for managing long-running pipelines with many sequential batches where maintaining a self-propagating job chain allows the entire dataset to be processed automatically while controlling the rate at which jobs are submitted to the cluster, avoiding overwhelming the scheduler or exceeding user job limits.
