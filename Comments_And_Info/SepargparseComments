This bash script orchestrates the beginning of the GALEX data processing pipeline as a high-performance computing job using the SLURM workload manager. The script requests 94 CPU cores on a single compute node with a 24-hour time limit on the windfall partition, and it's designed to run as an array job where each array task processes a different batch of sky coordinates. When the job starts, it loads Python 3.9 and activates a virtual environment, then calculates a file index number by subtracting one from the SLURM array task ID to match zero-based indexing. The script constructs paths to create a batch-specific directory under raw_files and to read an input coordinate file containing groups of four sky positions, creating the batch directory if it doesn't exist or cleaning and recreating it if it already exists. The core processing loop iterates through four coordinate sets in the input file, where for each set of coordinates it sequentially executes three Python scripts with timing information: first query.py downloads raw GALEX data from the MAST archive for the specified coordinates, then preprocessing.py filters out low-quality images and applies corrections like masking bad pixels and smoothing intensity data, and finally object_detection.py performs segmentation to detect and remove extended sources through Poisson noise infilling followed by point source detection and masking. Each script execution is timed and wrapped with date stamps to monitor processing duration and identify potential bottlenecks, with the srun command ensuring proper resource allocation within the SLURM environment. This automated pipeline design allows for efficient parallel processing of multiple sky regions across different array jobs, with each job independently handling a batch of four coordinate positions from start to finish without manual intervention.
